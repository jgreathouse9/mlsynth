# Using Least Squares {#sec-usels}


::: {.epigraph}
> "*Hey, baby, I really do, really do j'adore you... But you're so complicated.*"  
> — [*Neo Jessica Joshua*](https://genius.com/Mura-masa-and-nao-complicated-lyrics)

> *"You're the worst, you know what you've done to me."*  
> — [*Jhené Aiko Efuru Chilombo*](https://genius.com/Jhene-aiko-the-worst-lyrics)
:::


After reviewing DID as a method to generate counterfactuals, we saw that it implicitly assigns equal weight to all control units. In other words, DID assumes that each control unit should contribute identically to the counterfactual outcome of the treated unit, and that the baseline differences are enough to account for differences in trends. While this assumption simplifies estimation, it is often unrealistic. Control units can differ widely in how closely they resemble the treated unit before treatment, so treating them all as equally informative may distort the counterfactual.

To address this limitation, we need a method for weighting the donor pool $j \in \mathcal{N}_{0}$ in a way that reflects each unit’s similarity to the treated unit. One natural idea is to use ordinary least squares (OLS): choose weights that minimize the squared difference between the treated unit’s pre-treatment outcomes and a weighted combination of the donors. This formulation is exactly what synthetic control builds upon. In the next chapters, we’ll see how SCM modifies this OLS problem by adding constraints to the weights—turning this unconstrained regression into a more interpretable and robust estimator.

## The Model


Formally, if $\mathbf{y}_1$ is the vector of pre-treatment outcomes for the treated unit and $\mathbf{Y}_0$ is the matrix of pre-treatment outcomes for the donor units, we can define the OLS problem as:

$$
\mathbf{w}^{\ast} = \underset{\mathbf{w} \in \mathbb{R}^{N_0}}{\operatorname*{argmin}} \; \| \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} \|_2^2,
\qquad t \in \mathcal{T}_1
$$



Here, the resulting weights $\mathbf{w}^\ast$ can then be used to construct a synthetic control, both pre- and post-treatment, by taking a weighted sum of the donor outcomes. In essence, OLS assigns different weights to control units according to their similarity to the treated unit. We can examine the classic example of the Basque Country in Spain, which experienced a surge in terrorism in the 1970s due to the terrorist organization ETA murdering politicians and doing kidnappings. Suppose we want to estimate what Basque GDP per capita would have been in the absence of terrorism. Using OLS, we can easily construct a synthetic Basque by finding the combination of other Spanish regions whose weighted average best reproduces the Basque GDP path in the pre-1975 period. When we solve the OLS problem, each donor region receives a weight that minimizes the squared difference between its weighted combination and the Basque trajectory before the intervention. Intuitively, regions whose economic paths closely track the Basque Country are given higher weights, while less similar regions receive lower weights.

::: {.callout-note title="Programming Note"}

This is the first chapter in which we will cease doing the math by hand. Although OLS has a closed-form solution, the problem we consider here has about 20 observations and 16 predictors. You may do this by hand if you wish, but that will likely take much longer than you'd want to spend. So, from here on out, `cvxpy` and `numpy` will be your best friends here.
:::

## Counterfactuals Generated by OLS

```{python}

#| echo: True

import cvxpy as cp
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from mlsynth.utils.datautils import dataprep
from IPython.display import display, Markdown
from style import set_book_style
set_book_style()

url = ("https://raw.githubusercontent.com/"
       "jgreathouse9/mlsynth/"
       "refs/heads/main/basedata/"
       "basque_data.csv")


data = pd.read_csv(url)

prepped_data = dataprep(data, 
"regionname", 
"year", 
"gdpcap", 
"Terrorism")

y, Y0, T0, time = prepped_data["y"], prepped_data["donor_matrix"], prepped_data["pre_periods"], prepped_data['time_labels']

# Extract Ywide
Ywide = prepped_data["Ywide"]

subset = Ywide.iloc[:10, :4]

# Round to 2 decimal places
subset_rounded = subset.round(2)

# Convert to markdown
md_table = subset_rounded.to_markdown(index=True)

# Display
display(Markdown(md_table))

```

When we use OLS to construct a synthetic control, nothing mysterious is happening. We are literally just running a linear regression of the treated unit on the donor units, using only pre-treatment data. In this setup, the regression coefficients – the betas – are exactly the weights $\mathbf{w}$ that tell us how much each donor contributes to the synthetic control.

Formally, if $\mathbf{y}_1$ is a $T_0 \times 1$ vector of the treated unit’s outcomes in the pre-treatment period and $\mathbf{Y}_0$ is a $T_0 \times J$ matrix of donor outcomes, the OLS weights $\mathbf{w}^\ast$ are the solution to the normal equations $\mathbf{Y}_0^\top \mathbf{Y}_0  \mathbf{w}^\ast = \mathbf{Y}_0^\top \mathbf{y}_1$. When we solve, we get: $\mathbf{w}^\ast = (\mathbf{Y}_0^\top \mathbf{Y}_0)^{-1} \mathbf{Y}_0^\top \mathbf{y}_1$. This is just the standard formula for OLS regression coefficients. The solution minimizes the sum of squared differences between the treated unit and the weighted combination of donors over the pre-treatment period. We can compute the OLS weights directly in Python using `numpy` or `cvxpy`:

```{python}
# -----------------------------
# 1. Restrict to pre-treatment
# -----------------------------
Y0_pre = Y0[:T0, :]
y_pre = y[:T0]

# -----------------------------
# 2. NumPy solution (normal equations)
# -----------------------------
w_numpy = np.linalg.inv(Y0_pre.T @ Y0_pre) @ (Y0_pre.T @ y_pre)

# -----------------------------
# 3. CVXPY solution
# -----------------------------
w = cp.Variable(Y0_pre.shape[1])
objective = cp.Minimize(cp.norm(y_pre - Y0_pre @ w, 2))
problem = cp.Problem(objective)
problem.solve(solver=cp.CLARABEL)

w_cvxpy = w.value

# -----------------------------
# 4. Combine and label
# -----------------------------
donor_names = prepped_data["donor_names"]

df_weights = pd.DataFrame({
    "Donor": donor_names,
    "NumPy": np.round(w_numpy, 3).flatten(),
    "CVXPY": np.round(w_cvxpy, 3).flatten()
})

# -----------------------------
# 5. Convert to Markdown and display
# -----------------------------
md_table = df_weights.to_markdown(index=False)
display(Markdown(md_table))


```

See? Same weights! No tricks, no fancy adjustments, just plain old least squares. Let's look at who gets selected:


```{python}

#| echo: False

Ywide_data = prepped_data["Ywide"]

y = Ywide_data["Basque"].values
Y0two = Ywide_data[["Andalucia", "Castilla Y Leon"]].values
time = prepped_data["time_labels"]   # actual time labels
T0 = prepped_data["pre_periods"]     # number of pre-treatment periods

# Extract series
basque = Ywide_data["Basque"].values
andalucia = Ywide_data["Andalucia"].values
castilla = Ywide_data["Castilla Y Leon"].values
cataluna = Ywide_data["Cataluna"].values
plt.plot(time, basque, color="black", linewidth=2, label="Basque Country")
plt.plot(time, andalucia, color="blue", linestyle="--", label="Andalucia (w ≈ 4.18)")
plt.plot(time, castilla, color="green", linestyle="--", label="Castilla Y Leon (w ≈ 6.08)")
plt.plot(time, cataluna, color="#B22222", linestyle="--", label="Cataluna (w ≈ -0.95)")

# Mark intervention
plt.axvline(x=time[T0], color="grey", linestyle="--", label="Terrorism Begins")

# Labels
plt.xlabel("Year")
plt.ylabel("GDP per capita")
plt.title("Basque Country vs Selected Donors")
plt.legend()
plt.show()
```

Much of this does not really square very nicely with intuition. Cataluna, which is very close to the Basque Country (both literally in terms of GDP and being a French border region in Northern Spain) gets negative weight from OLS. The units which are much farther away from the Basque Country (on top of having much different political/economic histories) get much more weight. The reason for this is simple: OLS cares about minimizing errors first and foremost, and will assign any kind of weight it needs to pursuant to that end. This is the unconstrained minimizer, subject to no constraints upon the weights other than them being on the real line. Either way, we can still compute a counterfactual. Let's recall our chapter on averaging: all a weighted average is, is where the weight assigned per donor/element may vary. In the DID estimator we do

$$
\beta^\ast = \operatorname*{argmin}_{\beta \in \mathbb{R}} 
\left\| 
\underbrace{ 
\mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} 
}_{\text{Fit}} 
- 
\underbrace{ 
\beta 
}_{\text{Mean difference}} 
\right\|_2^2 
\quad \text{s.t.} \quad w_j = \frac{1}{N_0}, \quad \forall t \in \mathcal{T}_{1}, \: \forall j \in \mathcal{N}_0
$$
where the arithmetic mean wins the day. Here, by contrast, the weights are assigned entirely through least squares (without an intercept term).

$$
\mathbf{w}^{\ast} = \underset{\mathbf{w} \in \mathbb{R}^{N_0}}{\operatorname*{argmin}} \; \| \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} \|_2^2,
\qquad t \in \mathcal{T}_1
$$
And, as per the definition of a weighted sum, here's what happens next: we take the weight for Andalucia and multiply it by all the values of the Andalucia vector. Then we do the same for Aragon, and the rest of the donors, scaling them by the weight OLS assigns. Finally, we sum these weighted vectors to obtain our synthetic counterfactual trajectory, or $\hat{y}^{\text{LS}}=\mathbf{Y}_0 \mathbf{w}^{\ast}.$ Let's do that now.

```{python}
#| echo: False

import matplotlib.pyplot as plt

# -----------------------------
# 6. Predict counterfactual (OLS synthetic control)
# -----------------------------
y_hat = Y0 @ w_cvxpy  # predicted outcome for treated unit

# -----------------------------
# 7. Define time index
# -----------------------------
years = np.arange(1955, 1955 + len(y))

# -----------------------------
# 8. Plot: Treated vs Synthetic
# -----------------------------
plt.figure(figsize=(10, 6))
plt.plot(years, y, label="Basque", linewidth=2.5, color='black')
plt.plot(years, y_hat, label="OLS Basque", linestyle="--", linewidth=2.5, color='blue')

# Add pre-treatment vertical line (1989 is treatment year)
plt.axvline(x=1955+T0, color="gray", linestyle="-", linewidth=1.5)
plt.text(1955+T0, plt.ylim()[1] * 0.95, "Terrorism", color="gray")

plt.xlabel("Year")
plt.ylabel("GDP per Capita")
plt.legend()
plt.grid(alpha=0.3)
plt.show()

```

Here is our prediction using OLS. Unfortunately, while OLS fits very well to the Basque Country before terrorism happened, the out-of-sample/post-1975 predicions exhibits a lot more variance than any control unit in the pre-treatment period did. OLS interpolates perfectly within the span of donor trajectories in-sample, but because the coefficients are unconstrained, extrapolation outside the pre-treatment window becomes unstable. The coefficients for the in-sample model swing wildly when applied to new data. We can see that the prediction line for the Basque country suggests that its economy... would have fallen off a cliff... had terrorism not happened? In other words, if the OLS estmate is to be taken seriously, the onset of terrorism saved the Basque economy... This does not seem like a sensible finding historically, or a very reasonable prediction statistically.

::: {.problem title="DID vs OLS" data-link="#problem-did-vs-ols"}
Explain in your own words why DID assigns equal weight to all control units, and why this assumption might be unrealistic when constructing counterfactuals.
:::

::: {.problem title="Understanding OLS Weights" data-link="#problem-understanding-ols-weights"}
Describe the role of OLS weights in constructing a synthetic control. How does OLS decide which donor units get larger weights?
:::

::: {.problem title="Negative Weights" data-link="#problem-negative-weights"}
1. Explain why OLS can produce negative weights.  
2. What are the implications of a negative weight when creating a counterfactual?
:::



::: {.problem title="Coding Pre-Treatment Fit" data-link="#problem-coding-pre-treatment-fit"}
Using the Basque Country dataset:

```{python}
#| eval: False
#| echo: true
import numpy as np
import pandas as pd
from mlsynth.utils.datautils import dataprep

# Load data
url = "https://raw.githubusercontent.com/jgreathouse9/mlsynth/refs/heads/main/basedata/basque_data.csv"
data = pd.read_csv(url)

# Prepare pre-treatment data
prepped_data = dataprep(data, "regionname", "year", "gdpcap", "Terrorism")
y, Y0, T0, time = prepped_data["y"], prepped_data["donor_matrix"], prepped_data["pre_periods"], prepped_data["time_labels"]

# TODO: compute OLS weights and plot pre-treatment fit
```

1. Compute OLS weights using either `numpy` or `cvxpy`.
2. Plot the treated unit against the OLS synthetic control in the pre-treatment period.
3. Comment on which donors dominate the pre-treatment fit. What is the Root Mean Squared Error?
:::

::: {.problem title="Post-Treatment Predictions" data-link="#problem-post-treatment-predictions"}

1. Extend your synthetic control to the post-treatment period.
2. Plot the post-treatment predictions and the actual outcomes.
3. Discuss any issues you observe with the OLS predictions after the intervention.
:::

::: {.problem title="Overfitting Concept" data-link="#problem-overfitting-concept"}

1. Define overfitting in the context of OLS synthetic controls.
2. Explain why overfitting occurs more readily when there are many donor units relative to pre-treatment periods.
:::

::: {.problem title="Simulated Overfitting" data-link="#problem-simulated-overfitting"}

```{python}
#| eval: False
#| echo: true
# Simulate data
np.random.seed(42)
n, p = 10, 5
X = np.random.randn(n, p)
true_w = np.array([1, 0.5, 0, 0, 2])
y = X @ true_w + np.random.randn(n) * 0.5

# TODO: fit unconstrained OLS and plot predicted vs true values
```

1. Fit an unconstrained OLS model to the data.
2. Plot the predicted values vs the true values.
3. Comment on the in-sample fit and potential issues if you were to predict new outcomes.
:::

## The Problem with OLS

The OLS approach to constructing synthetic controls, while straightforward, often produces unreliable counterfactuals, as seen in the Basque Country example. The erratic post-1975 predictions, which suggest an implausible economic collapse absent terrorism, highlight a fundamental issue: OLS prioritizes in-sample fit at the expense of out-of-sample validity. This section explores why OLS fails in this context and the statistical mechanisms driving these shortcomings. OLS is designed to minimize the sum of squared differences between the treated unit’s pre-treatment outcomes ($\mathbf{y}_1$) and the weighted combination of donor outcomes ($\mathbf{Y}_0 \mathbf{w}$):

$$
\mathbf{w}^{\ast} = \underset{\mathbf{w} \in \mathbb{R}^{N_0}}{\operatorname*{argmin}} \; \| \mathbf{y}_1 - \mathbf{Y}_0 \mathbf{w} \|_2^2,
\qquad t \in \mathcal{T}_1
$$

The closed-form solution, $\mathbf{w}^\ast_{\text{OLS}} = (\mathbf{Y}_0^\top \mathbf{Y}_0)^{-1} \mathbf{Y}_0^\top \mathbf{y}_1$, achieves this by assigning weights that best fit the pre-treatment data. However, this unconstrained optimization leads to overfitting to noise, where OLS captures both systematic patterns and idiosyncratic noise ($\varepsilon$) in the treated unit’s pre-treatment outcomes, producing a synthetic control that closely tracks the pre-treatment period but fails to generalize post-treatment. This explains the volatile “zig-zag” counterfactual in the Basque case, which lacks the stability of any individual donor unit.

Without restrictions, OLS can also assign positive, negative, or arbitrarily large weights to donor units, leading to counterintuitive results. In the Basque example, Cataluna—a region economically and geographically similar to the Basque Country—received a negative weight, while distant regions like Castilla y León were assigned large positive weights. These weights prioritize in-sample accuracy over economic or historical plausibility, undermining interpretability. Furthermore, the model $\mathbf{y}_1 = \mathbf{Y}_0 \mathbf{w} + \varepsilon$ is only an approximation of the true data-generating process. By fitting the pre-treatment data too closely, OLS produces a synthetic control that lacks predictive power outside the pre-treatment/in-sample period, resulting in absurd post-treatment predictions, such as the Basque economy being helped by terrorism.


## Where OLS Leaves Off


Even though we see that it produces complicated and in this case uninterpretable results, OLS is still the most natural starting point for counterfactual construction in the panel data setting: it learns weights that make the treated unit’s pre-treatment outcomes as close as possible to a combination of donors. But as we’ve seen, this freedom comes at a price. By allowing weights to take any real value, OLS can perfectly interpolate the pre-treatment data but at the cost of producing implausible, unstable counterfactuals. In this sense, OLS represents the *purest* form of fitting: a method with no guardrails to contain its predictions.

The next chapter introduces *penalized least squares*, where we rein in OLS by attaching a penalty to the size (or shape) of the weight vector. This penalty forces the model to balance two competing goals:

1. **Fit:** stay close to the treated unit’s pre-treatment outcomes.  
2. **Simplicity:** avoid overly large or erratic weights.

When we add these penalties, new geometries emerge. The $\ell_2$ penalty (ridge) keeps weights small but allows them all to stay active; the $\ell_1$ penalty (lasso) forces some weights to zero, yielding sparse synthetic controls. And much later on, when we go one step further—replacing soft penalties with hard constraints—we arrive at the synthetic control method. OLS, then, is the first stop in a larger journey: from *fitting everything* to *fitting responsibly*.

