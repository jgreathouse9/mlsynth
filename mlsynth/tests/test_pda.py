import pytest
import pandas as pd
import numpy as np
from typing import Dict, Any, List # Added List
from pydantic import ValidationError

from mlsynth.estimators.pda import PDA
from mlsynth.config_models import PDAConfig, BaseEstimatorResults
from mlsynth.exceptions import MlsynthDataError, MlsynthEstimationError # Added

@pytest.fixture
def sample_pda_data() -> pd.DataFrame:
    """Creates a sample DataFrame for PDA tests."""
    n_units = 5  # Increased from 3 to 5 (4 donors)
    n_periods = 15 # Increased from 10 to 15
    treatment_start_period = 10 # Treatment at period 10 (9 pre-periods)

    units = np.repeat(np.arange(1, n_units + 1), n_periods)
    times = np.tile(np.arange(1, n_periods + 1), n_units)
    
    np.random.seed(123) # for reproducibility
    outcomes = []
    for i in range(n_units):
        base_trend = np.linspace(start=5 + i*3, stop=20 + i*3, num=n_periods)
        noise = np.random.normal(0, 0.5, n_periods)
        outcomes.extend(base_trend + noise)

    data = {
        "ID": units,
        "Period": times,
        "Value": outcomes,
        "IsTreated": np.zeros(n_units * n_periods, dtype=int),
    }
    df = pd.DataFrame(data)

    df.loc[(df['ID'] == 1) & (df['Period'] >= treatment_start_period), 'IsTreated'] = 1
    
    return df

def test_pda_creation(sample_pda_data: pd.DataFrame) -> None:
    """Test PDA estimator creation."""
    config_dict: Dict[str, Any] = {
        "df": sample_pda_data,
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
        "display_graphs": False,
        "method": "fs",
    }
    try:
        config_obj = PDAConfig(**config_dict)
        estimator = PDA(config_obj)
        assert estimator is not None
        assert estimator.df.equals(sample_pda_data)
        assert estimator.treat == "IsTreated"
        assert estimator.time == "Period"
        assert estimator.outcome == "Value"
        assert estimator.unitid == "ID"
        assert estimator.method == "fs"
        assert not estimator.display_graphs
    except Exception as e:
        pytest.fail(f"PDA creation failed: {e}")

@pytest.mark.parametrize("method_name", ["fs", "LASSO", "l2"])
def test_pda_fit_smoke(sample_pda_data: pd.DataFrame, method_name: str) -> None:
    """Smoke test for PDA fit method for various PDA types."""
    config_dict: Dict[str, Any] = {
        "df": sample_pda_data,
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
        "display_graphs": False,
        "method": method_name,
    }
    config_obj = PDAConfig(**config_dict)
    estimator = PDA(config_obj)
    try:
        results = estimator.fit()
        assert isinstance(results, BaseEstimatorResults)
        assert results.effects is not None
        assert results.fit_diagnostics is not None
        assert results.time_series is not None
        assert results.method_details is not None
        assert results.method_details.name is not None
        
        # Adjust expected method name for 'l2' which returns "l2 relaxation"
        expected_method_in_results = "l2 relaxation" if method_name == "l2" else method_name
        assert results.method_details.name.lower() == expected_method_in_results.lower()

        # Check raw_results for method-specific structures if needed for smoke test
        assert isinstance(results.raw_results, dict)
        if method_name == "fs":
            assert "Betas" in results.raw_results
            assert isinstance(results.raw_results["Betas"], dict)
        elif method_name == "LASSO":
            assert "non_zero_coef_dict" in results.raw_results
            assert isinstance(results.raw_results["non_zero_coef_dict"], dict)
        elif method_name == "l2":
            assert "Betas" in results.raw_results
            assert isinstance(results.raw_results["Betas"], dict)

    except Exception as e:
        # Known issue: LASSO/l2 can sometimes fail with singular matrices on small/specific random data
        if method_name in ["LASSO", "l2"] and "singular matrix" in str(e).lower():
            pytest.skip(f"Skipping PDA {method_name} due to singular matrix error: {e}")
        pytest.fail(f"PDA fit failed for method '{method_name}': {e}")

def test_pda_invalid_method(sample_pda_data: pd.DataFrame) -> None:
    """Test PDA creation with an invalid method."""
    config_dict: Dict[str, Any] = {
        "df": sample_pda_data,
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
        "display_graphs": False,
        "method": "invalid_method_name", # Invalid method
    }
    # Assuming PDAConfig uses Literal for method, this will raise ValidationError
    # If PDAConfig allows any string, then PDA.__init__ will raise ValueError
    with pytest.raises((ValidationError, ValueError)):
        config_obj = PDAConfig(**config_dict)
        PDA(config_obj) # ValueError from PDA if config_obj passes

def test_pda_invalid_tau_type(sample_pda_data: pd.DataFrame) -> None:
    """Test PDA creation with an invalid type for tau."""
    config_dict: Dict[str, Any] = {
        "df": sample_pda_data,
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
        "display_graphs": False,
        "method": "fs",
        "tau": "not_a_number" # Invalid tau type
    }
    with pytest.raises(ValidationError): # Pydantic should catch type error
        PDAConfig(**config_dict)

# --- Further Input Validation Tests ---

def test_pda_missing_df_in_config(sample_pda_data: pd.DataFrame) -> None:
    """Test PDA creation with 'df' missing from config."""
    config_dict: Dict[str, Any] = {
        # "df": sample_pda_data, # Missing
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
    }
    with pytest.raises(ValidationError): # df is required by BaseEstimatorConfig
        PDAConfig(**config_dict)

def test_pda_df_not_dataframe(sample_pda_data: pd.DataFrame) -> None:
    """Test PDA creation with 'df' not being a DataFrame."""
    config_dict: Dict[str, Any] = {
        "df": "not_a_dataframe", # Invalid type
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
    }
    with pytest.raises(ValidationError): # Pydantic should catch type error
        PDAConfig(**config_dict)


@pytest.mark.parametrize("missing_col", ["IsTreated", "Period", "Value", "ID"])
def test_pda_missing_column_in_df(sample_pda_data: pd.DataFrame, missing_col: str) -> None:
    """Test PDA fit with a required column missing from the DataFrame.
    Pydantic config creation should pass, error expected during fit.
    """
    df_modified = sample_pda_data.copy()
    del df_modified[missing_col]

    config_dict: Dict[str, Any] = {
        "df": df_modified,
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
        "display_graphs": False,
        "method": "fs", # Added a default valid method
    }
    
    # Update the config_dict if the missing column is one of the key names
    # This ensures the config_dict itself is valid for Pydantic instantiation
    # The error is expected from fit() due to missing column in the DataFrame
    if missing_col == "IsTreated":
        config_dict["treat"] = "IsTreated_missing" # Use a placeholder name
    elif missing_col == "Period":
        config_dict["time"] = "Period_missing"
    elif missing_col == "Value":
        config_dict["outcome"] = "Value_missing"
    elif missing_col == "ID":
        config_dict["unitid"] = "ID_missing"
    
    expected_error = MlsynthDataError
    missing_config_col_name = ""
    if missing_col == "IsTreated":
        missing_config_col_name = config_dict["treat"]
    elif missing_col == "Period":
        missing_config_col_name = config_dict["time"]
    elif missing_col == "Value":
        missing_config_col_name = config_dict["outcome"]
    elif missing_col == "ID":
        missing_config_col_name = config_dict["unitid"]
    match_pattern = f"Missing required columns in DataFrame 'df': {missing_config_col_name}"

    with pytest.raises(expected_error, match=match_pattern):
        PDAConfig(**config_dict) # Error is raised here by BaseEstimatorConfig validator

def test_pda_config_missing_essential_keys(sample_pda_data: pd.DataFrame) -> None:
    """Test PDA creation with essential keys missing from config (e.g., outcome)."""
    config_dict_missing_outcome: Dict[str, Any] = {
        "df": sample_pda_data,
        "treat": "IsTreated",
        "time": "Period",
        # "outcome": "Value", # Missing
        "unitid": "ID",
        "method": "fs", # Added a default valid method
    }
    with pytest.raises(ValidationError): # outcome is required by BaseEstimatorConfig
        PDAConfig(**config_dict_missing_outcome)

# --- Edge Case Tests ---

@pytest.mark.parametrize("method_name", ["fs", "LASSO", "l2"])
def test_pda_insufficient_pre_periods(sample_pda_data: pd.DataFrame, method_name: str) -> None:
    """Test PDA fit with insufficient pre-treatment periods."""
    # Scenario 1: 2 pre-periods
    df_modified_2_pre = sample_pda_data[sample_pda_data["Period"] >= 8].copy() 
    config_dict_2_pre: Dict[str, Any] = {
        "df": df_modified_2_pre,
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
        "display_graphs": False,
        "method": method_name,
    }
    # This config might pass for some methods, or fail gracefully.

    # Scenario 2: 1 pre-period (more likely to cause issues)
    df_modified_1_pre = sample_pda_data[sample_pda_data["Period"] >= 9].copy() 
    config_dict_1_pre: Dict[str, Any] = {
        "df": df_modified_1_pre,
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
        "display_graphs": False,
        "method": method_name,
    }
    config_obj_1_pre = PDAConfig(**config_dict_1_pre)
    estimator_1_pre = PDA(config_obj_1_pre)
    
    if method_name == "LASSO":
        # LassoCV(cv=min(T0_pre, 5)) with T0_pre=1 -> cv=1, which is invalid.
        # This will be wrapped by MlsynthEstimationError.
        with pytest.raises(MlsynthEstimationError, match="PDA estimation failed"):
            estimator_1_pre.fit()
    elif method_name == "fs":
        # PDAfs with T0_pre=1 will likely lead to singular matrix in inv(X.T @ X)
        # This will be wrapped by MlsynthEstimationError.
        with pytest.raises(MlsynthEstimationError, match="Singular matrix"):
            estimator_1_pre.fit()
    elif method_name == "l2":
        # cross_validate_tau with T0_pre=1 -> half_n_pre=0. l2_relax with 0 time periods.
        # This will be wrapped by MlsynthEstimationError.
        with pytest.raises(MlsynthEstimationError, match="Solver .* failed"): # CVXPY SolverError
            estimator_1_pre.fit()

@pytest.mark.parametrize("method_name", ["fs", "LASSO", "l2"])
def test_pda_insufficient_donors(sample_pda_data: pd.DataFrame, method_name: str) -> None:
    """Test PDA fit with insufficient donor units (e.g., zero donors)."""
    # Keep only the treated unit (ID 1)
    df_no_donors = sample_pda_data[sample_pda_data["ID"] == 1].copy()
    
    config_dict: Dict[str, Any] = {
        "df": df_no_donors,
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
        "display_graphs": False,
        "method": method_name,
    }
    config_obj = PDAConfig(**config_dict)
    estimator = PDA(config_obj)
    # dataprep should raise MlsynthDataError if there are no control units
    with pytest.raises(MlsynthDataError, match="No donor units found after pivoting and selecting."):
        estimator.fit()

@pytest.mark.parametrize("method_name", ["fs", "LASSO", "l2"])
def test_pda_no_post_periods(sample_pda_data: pd.DataFrame, method_name: str) -> None:
    """Test PDA fit with no post-treatment periods."""
    # Treatment starts at period 10. Keep data only up to period 9.
    df_no_post = sample_pda_data[sample_pda_data["Period"] < 10].copy()
    
    config_dict: Dict[str, Any] = {
        "df": df_no_post,
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
        "display_graphs": False,
        "method": method_name,
    }
    config_obj = PDAConfig(**config_dict)
    estimator = PDA(config_obj)
    # logictreat (via dataprep) will raise MlsynthDataError
    with pytest.raises(MlsynthDataError, match="No treated units found"):
        estimator.fit()


@pytest.mark.parametrize("method_name", ["fs", "LASSO", "l2"])
def test_pda_nan_in_outcome_treated(sample_pda_data: pd.DataFrame, method_name: str) -> None:
    """Test PDA fit with NaN in the outcome of the treated unit."""
    df_modified = sample_pda_data.copy()
    # Introduce NaN in pre-period and post-period for treated unit
    df_modified.loc[(df_modified["ID"] == 1) & (df_modified["Period"] == 5), "Value"] = np.nan
    df_modified.loc[(df_modified["ID"] == 1) & (df_modified["Period"] == 12), "Value"] = np.nan

    config_dict: Dict[str, Any] = {
        "df": df_modified,
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
        "display_graphs": False,
        "method": method_name,
    }
    config_obj = PDAConfig(**config_dict)
    estimator = PDA(config_obj)
    
    # Behavior with NaNs can vary.
    # dataprep might convert to 0 or drop, or the underlying algorithm might fail.
    # Assuming underlying algorithms (LassoCV, PDAfs, l2_relax) might raise errors or produce NaNs.
    # dataprep fills NaNs with 0 in donor_matrix, but not in y_treated_full.
    # So, y_treated_full (which becomes y_pre_lasso) will have NaNs.
    # These errors should be wrapped into MlsynthEstimationError by fit().
    if method_name == "LASSO":
        with pytest.raises(MlsynthEstimationError, match="Input y contains NaN."):
            estimator.fit()
    elif method_name == "fs":
        # PDAfs receives y_treated_full with NaNs.
        with pytest.raises(MlsynthEstimationError): # Original error was TypeError, now wrapped
             estimator.fit()
    elif method_name == "l2":
        # l2_relax receives y_treated_full with NaNs.
        # If estutils.pda (l2_relax part) handles NaNs by raising an error, it will be caught.
        # If it propagates NaNs leading to CVXPY failure, MlsynthEstimationError.
        # If it somehow completes, check for NaN results.
        try:
            results = estimator.fit()
            assert isinstance(results, BaseEstimatorResults) 
            assert results.effects is not None
            # ATT could be None if calculation fails gracefully, or NaN if it proceeds with NaNs
            assert results.effects.att is None or np.isnan(results.effects.att) # This line was already corrected in the previous user message.
        except MlsynthEstimationError:
            pass # This is also an acceptable outcome if NaNs cause solver failure


@pytest.mark.parametrize("method_name", ["fs", "LASSO", "l2"])
def test_pda_nan_in_outcome_donor(sample_pda_data: pd.DataFrame, method_name: str) -> None:
    """Test PDA fit with NaN in the outcome of a donor unit."""
    df_modified = sample_pda_data.copy()
    # Introduce NaN in pre-period and post-period for a donor unit (ID 2)
    df_modified.loc[(df_modified["ID"] == 2) & (df_modified["Period"] == 5), "Value"] = np.nan
    df_modified.loc[(df_modified["ID"] == 2) & (df_modified["Period"] == 12), "Value"] = np.nan

    config_dict: Dict[str, Any] = {
        "df": df_modified,
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
        "display_graphs": False,
        "method": method_name,
    }
    config_obj = PDAConfig(**config_dict)
    estimator = PDA(config_obj)
    
    # dataprep fills NaNs with 0 in donor_matrix, but not in y_treated_full.
    # So, donor_matrix_pre_lasso will have NaNs.
    # These errors should be wrapped into MlsynthEstimationError by fit().
    if method_name == "LASSO":
        with pytest.raises(MlsynthEstimationError, match="Input X contains NaN."):
            estimator.fit()
    elif method_name == "fs":
        # PDAfs receives donor_matrix_full with NaNs.
        with pytest.raises(MlsynthEstimationError): # Original error was TypeError, now wrapped
            estimator.fit()
    elif method_name == "l2":
        # l2_relax receives donor_matrix_full with NaNs.
        with pytest.raises(MlsynthEstimationError, match="Solver .* failed"): # CVXPY error wrapped
            estimator.fit()

# --- Detailed Results Validation Tests ---

@pytest.mark.parametrize("method_name", ["fs", "LASSO", "l2"])
def test_pda_results_structure_and_types(sample_pda_data: pd.DataFrame, method_name: str) -> None:
    """Test the detailed structure and types of the results dictionary for each method."""
    config_dict: Dict[str, Any] = {
        "df": sample_pda_data,
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
        "display_graphs": False,
        "method": method_name,
    }
    config_obj = PDAConfig(**config_dict)
    estimator = PDA(config_obj)
    results = estimator.fit()

    assert isinstance(results, BaseEstimatorResults)
    
    # Common Pydantic model checks
    assert results.effects is not None
    assert results.effects.att is None or isinstance(results.effects.att, (float, np.floating)) # Allow ATT to be None

    assert results.fit_diagnostics is not None
    # Check some specific fields if they are directly mapped, or in other_diagnostics
    # Example: if "R-Squared" is mapped to results.fit_diagnostics.r_squared
    if results.fit_diagnostics.r_squared is not None:
         assert isinstance(results.fit_diagnostics.r_squared, (float, np.floating))
    # Check other_diagnostics for keys like "T0 RMSE", "Pre-Periods", "Post-Periods"
    if results.fit_diagnostics.other_diagnostics:
        observed_fit_keys = ["Post-Periods", "Pre-Periods", "T0 RMSE"] # "R-Squared" might be here or direct
        if "R-Squared" not in results.fit_diagnostics.model_fields_set: # Check if R-Squared is not a direct field
             observed_fit_keys.append("R-Squared")

        for metric in observed_fit_keys:
            if metric in results.fit_diagnostics.other_diagnostics:
                assert isinstance(results.fit_diagnostics.other_diagnostics[metric], (float, np.floating, int))

    assert results.time_series is not None
    ts_series_map = {
        "Observed Unit": results.time_series.observed_outcome,
        "Counterfactual": results.time_series.synthetic_outcome,
        "Gap": results.time_series.treatment_effect_timeseries,
    }
    n_total_periods = sample_pda_data["Period"].nunique()
    
    for series_name, series_data in ts_series_map.items():
        assert series_data is not None
        assert isinstance(series_data, np.ndarray)
        assert len(series_data) == n_total_periods
        # If Gap is a list of lists (original was (T_total, 2))
        if series_name == "Gap":
             # The current Pydantic model for treatment_effect_timeseries is List[float]
             # The original "Gap" from pda_estimator_func is (T_total, 2)
             # My mapping in pda.py for "Gap" takes it as is.
             # If results.raw_results["Vectors"]["Gap"] is (T_total, 2)
             # then results.time_series.treatment_effect_timeseries will be a list of lists or similar.
             # Let's check raw_results for the original shape.
             assert isinstance(results.raw_results["Vectors"]["Gap"], np.ndarray)
             assert results.raw_results["Vectors"]["Gap"].shape == (n_total_periods, 2)
             # The Pydantic field treatment_effect_timeseries is List[float], so it must be one of the columns.
             # Assuming it's the first column of the Gap.
        else: # Observed Unit, Counterfactual
            assert all(isinstance(x, (float, np.floating, int)) for x in series_data)


    assert results.method_details is not None
    assert results.method_details.name is not None
    expected_method_in_results = "l2 relaxation" if method_name == "l2" else method_name
    assert results.method_details.name.lower() == expected_method_in_results.lower()

    # Access method-specific data from results.raw_results
    raw = results.raw_results
    assert isinstance(raw, dict)

    if method_name == "fs":
        assert "Betas" in raw and isinstance(raw["Betas"], dict)
        for donor_name, coeff_val in raw["Betas"].items():
            assert isinstance(donor_name, (str, int, float, np.integer, np.floating))
            assert isinstance(coeff_val, (float, np.floating))
        assert "Inference" in raw and isinstance(raw["Inference"], dict)
        inference_keys_fs = ["t_stat", "SE", "95% CI", "p_value"]
        for i_key in inference_keys_fs:
            assert i_key in raw["Inference"]
            # Type checks for inference values
            if i_key == "95% CI":
                assert isinstance(raw["Inference"][i_key], tuple) and len(raw["Inference"][i_key]) == 2
            else:
                assert isinstance(raw["Inference"][i_key], (float, np.floating))

    elif method_name == "LASSO":
        assert "non_zero_coef_dict" in raw and isinstance(raw["non_zero_coef_dict"], dict)
        for donor_name, coeff_val in raw["non_zero_coef_dict"].items():
            assert isinstance(donor_name, (str, int, float, np.integer, np.floating))
            assert isinstance(coeff_val, (float, np.floating))
        assert "Inference" in raw and isinstance(raw["Inference"], dict)
        inference_keys_lasso = ["CI", "t_stat", "SE"]
        for i_key in inference_keys_lasso:
            assert i_key in raw["Inference"]
            if i_key == "CI":
                 assert isinstance(raw["Inference"][i_key], tuple) and len(raw["Inference"][i_key]) == 2
            else:
                assert isinstance(raw["Inference"][i_key], (float, np.floating))

    elif method_name == "l2":
        assert "Betas" in raw and isinstance(raw["Betas"], dict)
        for donor_name, coeff_val in raw["Betas"].items():
            assert isinstance(donor_name, (str, int, float, np.integer, np.floating))
            assert isinstance(coeff_val, (float, np.floating))
        assert "Inference" in raw and isinstance(raw["Inference"], dict)
        inference_keys_l2 = ["t_stat", "standard_error", "p_value", "confidence_interval"]
        for i_key in inference_keys_l2:
            assert i_key in raw["Inference"]
            if i_key == "confidence_interval":
                assert isinstance(raw["Inference"][i_key], tuple) and len(raw["Inference"][i_key]) == 2
            else:
                assert isinstance(raw["Inference"][i_key], (float, np.floating))
        assert "optimal_tau" in raw and isinstance(raw["optimal_tau"], (float, np.floating))
        assert "Intercept" in raw and isinstance(raw["Intercept"], (float, np.floating))

# --- Configuration Variation Tests ---

@pytest.mark.parametrize("tau_value", [0.1, 1.0, None]) # Test different tau values for L2
def test_pda_l2_with_tau_config(sample_pda_data: pd.DataFrame, tau_value: Any) -> None: # Changed type hint for tau_value
    """Test L2 method with different 'tau' configurations."""
    config_dict: Dict[str, Any] = {
        "df": sample_pda_data,
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
        "display_graphs": False,
        "method": "l2",
        "tau": tau_value, 
    }
    config_obj = PDAConfig(**config_dict)
    estimator = PDA(config_obj)
    results = estimator.fit()
    assert isinstance(results, BaseEstimatorResults)
    assert results.method_details is not None
    assert results.method_details.name is not None
    assert results.method_details.name.lower() == "l2 relaxation"
    
    assert results.raw_results is not None # optimal_tau is in raw_results
    if tau_value is not None:
        assert results.raw_results["optimal_tau"] == tau_value
    else:
        assert isinstance(results.raw_results["optimal_tau"], float)

# --- Plotting Behavior Tests ---
from unittest.mock import patch

@pytest.mark.parametrize("display_graphs_flag", [True, False])
@pytest.mark.parametrize("method_name", ["fs", "LASSO", "l2"])
def test_pda_plotting_behavior(sample_pda_data: pd.DataFrame, display_graphs_flag: bool, method_name: str) -> None:
    """Test plotting behavior based on display_graphs flag."""
    config_dict: Dict[str, Any] = {
        "df": sample_pda_data,
        "treat": "IsTreated",
        "time": "Period",
        "outcome": "Value",
        "unitid": "ID",
        "display_graphs": display_graphs_flag,
        "method": method_name,
        "save": False # Ensure save doesn't interfere
    }
    
    with patch("mlsynth.estimators.pda.plot_estimates") as mock_plot:
        config_obj = PDAConfig(**config_dict)
        estimator = PDA(config_obj)
        estimator.fit()
        if display_graphs_flag:
            mock_plot.assert_called_once()
            call_args = mock_plot.call_args[1] # Get kwargs
            assert call_args['df'] is not None # prepped dict
            assert call_args['time'] == config_dict['time']
            assert call_args['unitid'] == config_dict['unitid']
            # ... other relevant args
        else:
            mock_plot.assert_not_called()
